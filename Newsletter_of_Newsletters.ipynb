{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Newsletter_of_Newsletters.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATQUbfWr9Fee",
        "colab_type": "code",
        "outputId": "18ffb1fd-2e73-43e0-efba-0fd6f5089578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install requests                 #Requesting the URL\n",
        "!pip install bs4                      #Web Scrapping \n",
        "!pip install validator_collection     #Validator the conditions \n",
        "!pip install git+\"https://github.com/codelucas/newspaper.git\"\n",
        "!pip install goose3 #Webwrapper Package\n",
        "!pip install nltk\n",
        "!pip install sumy\n",
        "!pip install easy-rouge"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n",
            "Collecting validator_collection\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/8c/52fbb84e7f9b5b5da2251d598a40414ef04f963f4774b6b14ca94cbf51f0/validator_collection-1.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from validator_collection) (2.6.0)\n",
            "Installing collected packages: validator-collection\n",
            "Successfully installed validator-collection-1.4.1\n",
            "Collecting git+https://github.com/codelucas/newspaper.git\n",
            "  Cloning https://github.com/codelucas/newspaper.git to /tmp/pip-req-build-57bt2y4i\n",
            "  Running command git clone -q https://github.com/codelucas/newspaper.git /tmp/pip-req-build-57bt2y4i\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k==0.2.8) (4.6.3)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 4.2MB/s \n",
            "\u001b[?25hCollecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 56.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k==0.2.8) (4.2.6)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k==0.2.8) (3.2.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k==0.2.8) (7.0.0)\n",
            "Collecting pythainlp>=1.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/14/b80930a2cc09ed6b5f8a22da9be6ece56939839ae66d921d9c7123034ba0/pythainlp-2.1.4-py3-none-any.whl (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k==0.2.8) (2.8.1)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k==0.2.8) (3.13)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k==0.2.8) (2.21.0)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from feedfinder2>=0.0.4->newspaper3k==0.2.8) (1.12.0)\n",
            "Collecting tinydb>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/22/11/c3adfc1b367d1955461f82a4a0a8ffffd37b193e98f2fe89338cdd4a8a6a/tinydb-3.15.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dill>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from pythainlp>=1.7.2->newspaper3k==0.2.8) (0.3.1.1)\n",
            "Requirement already satisfied: tqdm>=4.1 in /usr/local/lib/python3.6/dist-packages (from pythainlp>=1.7.2->newspaper3k==0.2.8) (4.38.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k==0.2.8) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k==0.2.8) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k==0.2.8) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k==0.2.8) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k==0.2.8) (46.0.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: newspaper3k, feedfinder2, feedparser, jieba3k, tinysegmenter\n",
            "  Building wheel for newspaper3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for newspaper3k: filename=newspaper3k-0.2.8-cp36-none-any.whl size=214639 sha256=36b967f7cafde6897704e91196ea128881c46c5c485789153935f8db312d9258\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mgpm2sr/wheels/9c/bf/af/bb27f46a20721b29a9623b572b89928ebe96f255f398e63fa4\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3357 sha256=88b29c083308a7e847785498c4ce9ec496a3a05e9b3f7bf4e2cc7d0661290ca0\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=fe0fef94e795abe482dc9f5593f96e71cacd7c9c363a624fafd625787fb8bf43\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=cfa74fa1f552b19668d9783ee02695272444ef956e24e8498a7579a348f5dcab\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13539 sha256=67663268e4a3c3d36431300066796f87f3a773b665309732ae304adb73f1c06d\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "Successfully built newspaper3k feedfinder2 feedparser jieba3k tinysegmenter\n",
            "\u001b[31mERROR: pythainlp 2.1.4 has requirement nltk>=3.3, but you'll have nltk 3.2.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pythainlp 2.1.4 has requirement requests>=2.22.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cssselect, feedfinder2, feedparser, jieba3k, tinydb, pythainlp, tinysegmenter, requests-file, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 pythainlp-2.1.4 requests-file-1.4.3 tinydb-3.15.2 tinysegmenter-0.3 tldextract-2.2.2\n",
            "Collecting goose3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/0e/5d049211226268ebce83ae5c8c4f578af0f5f120b24de9542485efcfeda2/goose3-3.1.6-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from goose3) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from goose3) (4.2.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from goose3) (7.0.0)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.6/dist-packages (from goose3) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from goose3) (2.8.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from goose3) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from goose3) (2.21.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from goose3) (0.42.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->goose3) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->goose3) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->goose3) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->goose3) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->goose3) (2.8)\n",
            "Installing collected packages: goose3\n",
            "Successfully installed goose3-3.1.6\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Collecting sumy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/20/8abf92617ec80a2ebaec8dc1646a790fc9656a4a4377ddb9f0cc90bc9326/sumy-0.8.1-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from sumy) (2.21.0)\n",
            "Collecting pycountry>=18.2.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/b6/154fe93072051d8ce7bf197690957b6d0ac9a21d51c9a1d05bd7c6fdb16f/pycountry-19.8.18.tar.gz (10.0MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sumy) (3.2.5)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from sumy) (0.6.2)\n",
            "Collecting breadability>=0.1.20\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.0.2->sumy) (1.12.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy) (4.2.6)\n",
            "Building wheels for collected packages: pycountry, breadability\n",
            "  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-19.8.18-py2.py3-none-any.whl size=10627361 sha256=2839d453ececb09763732708483d58e0b38041e1c651dbbe610aec0b31d3e8c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/98/bf/f0fa1c6bf8cf2cbdb750d583f84be51c2cd8272460b8b36bd3\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21684 sha256=39d23d88245a738b0a5859ddd12c43b16376d0de658c7be05f7059969929bb64\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n",
            "Successfully built pycountry breadability\n",
            "Installing collected packages: pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 pycountry-19.8.18 sumy-0.8.1\n",
            "Collecting easy-rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/0a/b7ebb887dac3ece27fffc65bbc7dc0abcf991f2ccce8073126329ce4be8f/easy_rouge-0.2.2-py3-none-any.whl\n",
            "Installing collected packages: easy-rouge\n",
            "Successfully installed easy-rouge-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GYDMEKtFbK0-",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import requests\n",
        "from validator_collection import checkers\n",
        "import dateutil.parser\n",
        "from datetime import date\n",
        "\n",
        "# Pages from which the newsletters are scrapped\n",
        "## Update the pages on a weekly basis for weekly updates\n",
        "Pages=[\"https://awsinsider.net/Home.aspx\",\"https://dataengweekly.substack.com/\",\n",
        "       \"https://news.dataelixir.com/t/t-EBBA06B24152E04B2540EF23F30FEDED\",\n",
        "       \"https://www.datascienceweekly.org/newsletters/data-science-weekly-newsletter-issue-315\"]\n",
        "\n",
        "#Sources of the newsletters\n",
        "Sources=[\"awsinsider\",\"dataengweekly\",\n",
        "         \"dataelixir\",\"datascienceweekly\"]\n",
        "\n",
        "#Pages=[\"https://awsinsider.net/Home.aspx\",\n",
        "#       \"https://www.datascienceweekly.org/newsletters/data-science-weekly-newsletter-issue-315\"]\n",
        "\n",
        "#Sources of the newsletters\n",
        "#Sources=[\"awsinsider\",\"datascienceweekly\"]\n",
        "\n",
        "\n",
        "#Request \n",
        "links=[]\n",
        "#Looping the differenent webpages\n",
        "for i in range(0,len(Pages)):\n",
        "  # Sending request to each page\n",
        "   res = requests.get(Pages[i]) \n",
        "  # Taking all text value from the webpage  \n",
        "   Soup=BeautifulSoup(res.text,'lxml')\n",
        "   #Inside the webpage we have several other links\n",
        "   NewsLinks=[]\n",
        "   #Finding all the a tag in the webpage\n",
        "   for a in Soup.find_all('a', href=True):\n",
        "    #Checking is this a webpage url or not \n",
        "    if checkers.is_url(a['href']):\n",
        "      # if Webpage url is below vales then append\n",
        "      if Sources[i]==\"awsinsider\" and \"articles\" in a['href'] and a['href'] not in NewsLinks and re.search('\\d{4}/\\d{2}/\\d{2}|\\d{4}/\\w{3}/\\d{2}|\\d{2}/\\d{2}/\\d{4}|\\d{2}/\\w{3}/\\d{4}', a['href']):\n",
        "        NewsLinks.append(a['href'])\n",
        "      elif Sources[i]==\"dataengweekly\" and a['href'] not in NewsLinks:\n",
        "        NewsLinks.append(a['href'])\n",
        "      elif Sources[i]==\"dataengweekly\" and \"http://www.datadog.com\" in a['href'] :\n",
        "        break\n",
        "      elif Sources[i]==\"dataelixir\" and \"forwardtomyfriend\" in a['href']:\n",
        "        break\n",
        "      elif Sources[i]==\"dataelixir\": #and \"news.dataelixir\" not in a['href']:\n",
        "        NewsLinks.append(a['href'])\n",
        "      elif Sources[i]==\"dataelixir\" and \"threader.app\" not in a['href']:\n",
        "        NewsLinks.append(a['href'])\n",
        "      elif Sources[i]==\"datascienceweekly\" and \"dataform\" in a[\"href\"]:\n",
        "        break\n",
        "      elif Sources[i]==\"datascienceweekly\" and \"thisismetis\" not in a['href']:\n",
        "        NewsLinks.append(a['href'])\n",
        "      elif Sources[i]==\"datascienceweekly\" and \"threader.app\" not in a['href']:\n",
        "        NewsLinks.append(a['href'])\n",
        "   links.append(NewsLinks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxeSlHDTRc1E",
        "colab_type": "code",
        "outputId": "b17010ec-1466-44aa-91e0-439ea885671d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(links)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJJP-x3ILByE",
        "colab_type": "code",
        "outputId": "4ba5bf93-5519-474b-d668-4841c9bbb336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from newspaper import Article\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.summarization.summarizer import summarize\n",
        "import datetime\n",
        "import pytz\n",
        "utc=pytz.UTC\n",
        "from datetime import timezone\n",
        "from goose3 import Goose\n",
        "\n",
        "summary=[]\n",
        "link=[]\n",
        "title=[]\n",
        "keywords=[]\n",
        "date=[]\n",
        "Source=[]\n",
        "Group=[]\n",
        "Text=[]\n",
        "count=1\n",
        "g = Goose()\n",
        "for page in links:\n",
        "  summary_=[]\n",
        "  link_=[]\n",
        "  title_=[]\n",
        "  keywords_=[]\n",
        "  date_=[]\n",
        "  Source_=[]\n",
        "  text_=[]\n",
        "  #print(page)\n",
        "  for url in page:\n",
        "    try:\n",
        "      #article = g.extract(url)\n",
        "      #For different language newspaper refer above table \n",
        "      toi_article = Article(url, language=\"en\") # en for English \n",
        "    \n",
        "      #To download the article \n",
        "      toi_article.download() \n",
        "  \n",
        "      #To parse the article \n",
        "      toi_article.parse() \n",
        "    \n",
        "      #To perform natural language processing ie..nlp \n",
        "      toi_article.nlp() \n",
        "    \n",
        "      if len(toi_article.summary.splitlines())>=5 :#and #(toi_article.publish_date)>= (datetime.datetime.now() - datetime.timedelta(-7)):\n",
        "        if len(word_tokenize(toi_article.summary))>60:\n",
        "          summary_.append(summarize(toi_article.text.encode('utf8').decode('ascii', 'ignore'),word_count =60))\n",
        "        else:\n",
        "          summary_.append(toi_article.summary)\n",
        "        link_.append(url)\n",
        "        if toi_article.title!=\"Quanta Magazine\":\n",
        "          title_.append(toi_article.title)\n",
        "        else:\n",
        "          article = g.extract(url)\n",
        "          title_.append((article.title))\n",
        "        keywords_.append(toi_article.keywords)\n",
        "        date_.append(toi_article.publish_date)\n",
        "        text_.append(toi_article.text)\n",
        "        if count <= 2:\n",
        "          Source_.append(\"Data Engineering\")\n",
        "        else:\n",
        "          Source_.append(\"Data Science\")\n",
        "\n",
        "    except:\n",
        "      continue\n",
        "      \n",
        "    #try:|\n",
        "    #  \n",
        "    #except:\n",
        "    #  title_.append((toi_article.title))\n",
        "  count=count+1\n",
        "  summary.append(summary_)\n",
        "  link.append(link_)\n",
        "  title.append(title_)\n",
        "  keywords.append(keywords_)\n",
        "  date.append(date_)\n",
        "  Source.append(Source_)\n",
        "  Text.append(text_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKiPsVMTveI1",
        "colab_type": "code",
        "outputId": "799b5b84-b9d7-4f8b-e01d-3938cbe59dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "\n",
        "def Data_cleaning(text):\n",
        "    \n",
        "    text=text.lower()\n",
        "    \n",
        "    text=re.sub(\"<!--?.*?-->\",\"\",text)\n",
        "    #text=re.sub(\"\\u\",\"\",text)\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "\n",
        "    #text=re.sub('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~',\" \",text)\n",
        "\n",
        "    stop_words=set(stopwords.words('english'))\n",
        "\n",
        "    words=word_tokenize(text)\n",
        "\n",
        "    punctuated_sentence=[]\n",
        "\n",
        "    for i in words:\n",
        "        if i not in string.punctuation:\n",
        "            punctuated_sentence.append(i)\n",
        "          \n",
        "    Preprocessed_data=\"\"\n",
        "    for i in (punctuated_sentence):\n",
        "      Preprocessed_data=Preprocessed_data+\" \"+i\n",
        "      \n",
        "    return Preprocessed_data\n",
        "  \n",
        "cleaned_blog=[]  \n",
        "for blog in ((Text)):\n",
        "  cleaned_article=[]\n",
        "  for article in ((blog)):\n",
        "    cleaned_article.append(Data_cleaning(article))\n",
        "  cleaned_blog.append(cleaned_article)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LgZw6otyO84",
        "colab_type": "code",
        "outputId": "8735b4bf-130d-4316-aebe-63a9e4e6de67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "cleaned_blog[1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' this week s issue has posts from scribd and slack on apache airflow using envoy with apache kafka the open sourcing of linkedin s datahub elasticsearch in production and apache flink s new sql ddl support also a post on the data infrastructure behind the spotify wrapped campaign and an article with advice on running a data team slack wrote about their experiences upgrading apache airflow from version which they had been running for two years to version the post describes the upgrade strategies that they considered the steps they took many around schema changes and backing up the metadata database how they tested the upgrade and some issues they found after the upgrade https slack engineering reliably upgrading apache airflow at slacks scale a f d a spotify writes about their large scale analysis of a decade of playback data to power their spotify wrapped campaign at the end of they performed a number of intermediate jobs which allowed them to more quickly iterate and verify the quality of outputs they talk about some of the changes they made since s campaign including changing the way that they store data in order to avoid large amounts of shuffling and thus higher processing costs https labs spotify com wrapping up the decade a data story scribd writes about their journey from a home grown workflow engine to apache airflow their main dag has over tasks there s a fun visualization in the post so it s a big undertaking to make the move the post describes the main motivators and some of the high level changes they ve made to move to airflow https tech scribd com blog modernizing an old data pipeline html a mix of technical and managerial advice this post shares lessons learned from running the data team at gitlab for a year technical topics include how to choose the right tools including strategically buying some products and investing in process tools for onboarding and if you re on the manager side there s a bunch of advice about how big your team should be how to get executive buy in and more https about gitlab com blog lessons learned as data team manager an introduction to using envoy as a reverse proxy for apache kafka traffic the post describes how to configure an envoy filter to gather metrics on requests responses for client traffic only or all traffic including requests for inter broker replication https medium com adam kotwasinski deploying envoy and kafka aa ec a linkedin has open sourced datahub their tool for metadata management of data platforms while the technical details of datahub were covered in previous posts this article describes how linkedin plans to maintain the code both internally and as an open source project and also how the features of the two versions differ https engineering linkedin com blog open sourcing datahub linkedins metadata search and discovery p apache flink adds new sql ddl syntax for configuring data sources and sinks the post has some examples for defining new tables e g kafka and elasticsearch and details on flink s catalog system https flink apache org news ddl html the morning paper has coverage of a paper on microsoft s raven which embeds ml runtimes into sql server the idea is to keep models as data in the database so that you can take advantage of features like transactions and improve performance pretty interesting details including how they ve released as part of public preview in azure sql database edge https blog acolyer org extending relational query processing an in depth look at the architecture of elasticsearch towards the goal of planning and monitoring a production deployment https facinating tech in depth guide to running elasticsearch in production links are provided for informational purposes and do not imply endorsement all views expressed in this newsletter are my own and do not represent the opinions of current former or future employers',\n",
              " ' apache airflow is a tool for describing executing and monitoring workflows at slack we use airflow to orchestrate and manage our data warehouse workflows which includes product and business metrics and also is used for different engineering use cases e g search and offline indexing for two years we ve been running airflow and it was time for us to catch up with the recent releases and upgrade to airflow in this post we ll describe the problems we encountered and the solutions we put in place as of september slack has over million daily active users performing billion actions on average every week this results in over billion records loaded daily into our s data warehouse this data is then processed by hundreds of workflows running apache hive apache spark and presto on airflow that submit jobs to our clusters running on thousands of ec instances requirements of the upgrade reliability the airflow scheduler and webserver should run without issues after the upgrade we are running hundreds of workflows that are managing the state of thousands of tasks and all these should be scheduled and executed successfully fast rollback besides all the bug fixes and improvements that this new version brings it also involves a backwards incompatible schema upgrade on airflow s metadata database if things go wrong we want to be able to roll back to the previous airflow version and schema quickly minimized downtime we want to reduce the time airflow is down during the upgrade so we don t affect our airflow users and don t miss slas as folks rely on having data on time history preserved the metadata of previous runs should be preserved so that we could run backfills and don t have to update start_dates on the dags upgrade strategies we considered a couple strategies for the airflow upgrade red black upgrade we run old and new versions of airflow side by side and move a small set of workflows over at a time this is the more reliable option but it wasn t feasible to do this we would need each version of airflow to point to its own metadata database since sharing the same database can cause the same tasks to be scheduled on both airflows resulting in duplicates creating two databases for each and moving dags piecewise will result in losing history big bang upgrade we test as much as possible in dev and move all dags to the new version in one big bang if there are issues either fix forward or roll back the upgrade the challenge here was that we didn t have very good test data and had only a handful of dags in dev since the red black upgrade was not feasible and involved more risk to the data quality we went ahead with the big bang upgrade we added some of our critical dags from prod to dev for testing steps needed for the big bang upgrade following are the high level steps for the upgrade we did these steps in dev first and then in our prod environment launch an instance with airflow installed fix incompatibilities with libraries that are installed on top of airflow back up the airflow metadata db upgrade the schema start airflow services test and either fix forward or roll back to optimize the upgrade based on the initial requirements we considered some approaches for database backup and schema upgrade which we will delve into next database backup we wanted to upgrade the database in a way where we can rollback quickly and also minimize overall downtime we considered two approaches snapshot this is a pretty straightforward approach where first we take a snapshot of the master then upgrade to the new schema and roll back to the snapshot if we see issues however this will result in more catchup time and more downtime because taking a snapshot and restoring it takes a non trivial amount of time due to our airflow metadata db being quite large use replicas as the new master at the start of this process we had one master and one replica master replica in this approach we create two more replicas replica replica replicating from master when we are ready to upgrade the schema we cut replica off from replication and run the upgrade schema script this replica then becomes the new master if the upgrade fails we roll back immediately to the old master we decided to go with the second approach for db migration because we can rollback quickly with less downtime we reduce catch up time since we don t have to take snapshots schema upgrade airflow s metadata db schema has undergone many changes since version to get from to the general recommendation is to go to first otherwise we can not directly use the out of the box command included with airflow to upgrade the db airflow upgradedb however we didn t want to spend the time to do two upgrades and instead wanted to go directly to we also observed that the airflow upgradedb script which was written in python using the alembic package was taking a long time to run likely due to our production metadata db having over ten million rows in some tables to resolve all of this we wrote our own mysql schema upgrade script we noticed that running the same queries from mysql was much faster compared to the python scripts included in airflow we have contributed this script back to the community see https issues apache org jira browse airflow problems we encountered in the airflow upgrade and our solutions we found a few issues during testing and some after the upgrade issues found during testing adhoc attribute removal the adhoc attribute was removed from the task object in this previously told airflow not to run a task in a scheduled manner but rather marked it as a task that will only be run manually we had a few dags with adhoc tasks the solution was to consolidate those tasks in a new dag and to mark that dag as schedule_interval none the presto operator stopped working this is due to a more recent future package being incompatible with presto hook in airflow when it is run on python to resolve this we either need to convert the protocol field in presto hook to unicode or move airflow to python we went with the former approach since python migration is a big effort on its own and we don t want to have too many moving parts in the version upgrade ui issues that could create data quality issues in when we click mark success on a task instead of just marking the task as success it marks the task and all its downstream dependencies to success this could result in dependent dags kicking off and producing incorrect data as the user could mark by mistake tasks that they did not intend to we disabled mark success and instead use the task instance admin page to do the same issues found after the upgrade hiveserver hook stopped allowing describe queries this issue has been reported upstream in airflow to fix this we wrote our own slackhiveserver hook which allows describe boto vs boto there was a change in behavior of the aws s list_keys api in boto which caused some tasks to fail this was because list_keys started to ignore s folder markers folder files we resolved this by handling this new behavior appropriately in our library code timezone issues airflow became timezone aware this caused the execution dates in the metadata db to return results in the time zone that the mysql server or the host was configured to which was pacific time however some of our dashboards broke because they were expecting data in utc to fix this we updated the default timezone in airflow metadata db to utc lessons learned after the upgrade was complete we did a post mortem here are some of the lessons learnt from the upgrade positive takeaways runbook we created a runbook for the upgrade with the exact steps and commands that needed to be executed allowing us to simply copy and paste them this ensured we didn t make mistakes during the upgrade we created a runbook for the upgrade with the exact steps and commands that needed to be executed allowing us to simply copy and paste them this ensured we didn t make mistakes during the upgrade tech debt as part of the upgrade we reduced a large amount of tech debt that had been lingering in our code base as part of the upgrade we reduced a large amount of tech debt that had been lingering in our code base communication we sent out clear communication well in advance to our stakeholders about the upgrade the downtime and what to expect things we can do better frequent upgrades instead of doing upgrades once every two years we would like to do them more frequently so that we don t introduce large changes in one shot instead of doing upgrades once every two years we would like to do them more frequently so that we don t introduce large changes in one shot more dev dags we had issues caught post upgrade that didn t show up in dev because we had only a few dev dags we would like to increase that number so we get adequate coverage during testing we had issues caught post upgrade that didn t show up in dev because we had only a few dev dags we would like to increase that number so we get adequate coverage during testing better monitoring and alerting we had a number of internal user reports after the upgrade with questions like hey my dag is not scheduled or where is my data we want to know about these issues before users report them to us to solve this problem we recently created an alerting framework to monitor whether airflow is scheduling dags on time and alert us if it is not we had a number of internal user reports after the upgrade with questions like hey my dag is not scheduled or where is my data we want to know about these issues before users report them to us to solve this problem we recently created an alerting framework to monitor whether airflow is scheduling dags on time and alert us if it is not strategy to restart tasks right after the upgrade we bulk cleared a number of tasks which overloaded our emr cluster for the next upgrade we d like to have a strategy to clear critical tasks first followed by lower priority tasks right after the upgrade we bulk cleared a number of tasks which overloaded our emr cluster for the next upgrade we d like to have a strategy to clear critical tasks first followed by lower priority tasks internal code freeze during the upgrade we made some unrelated code changes to emr which could have waited for a day and these changes ended up breaking some dags we plan to have an internal code freeze when we do major upgrades and allow only upgrade related changes to be committed conclusion despite a few hiccups the upgrade was overall successful the issues that we found post upgrade were fixed forward and most of the critical dags caught up quickly the next step is to move airflow into kubernetes so that we can scale tasks horizontally this upgrade wouldn t have been possible without efforts from multiple teams at slack special thanks to deepak barge ananth packkildurai derek smith and atl arredondo we have a lot more challenging problems in scale reliability and security coming up in the data infrastructure team if you d like to help us please check out https slack com careers',\n",
              " ' the spotify wrapped campaign is one of spotify s largest marketing and social campaigns of the year it enables our users to see a detailed breakdown of their listening habits over the past year since was the end of the decade we wanted to do something special for our users as one of the few streaming platforms that has existed since before spotify had a unique opportunity to be able to provide users with a review of their listening habits over the entire decade this was an ambitious goal and one that posed many engineering challenges because wrapped is such a massive effort the wrapped team itself encompasses many sub teams that are responsible for everything from marketing legal and design to data frontend and backend engineering many of these teams are spun up in the months prior launch and are largely made up of volunteers who put aside their day to day work to focus entirely on launching wrapped in this post we will focus on how our team the user wrapped data engineering team managed to process x the amount of data compared to that of s wrapped campaign at a fraction of the cost scope for s wrapped there were multiple statistics that we wanted to highlight individually for our users in this post we will refer to each of these separate statistics as a data story because it is a statistic or story we are telling the user through their historical listening data for example your top artists your top songs and your top podcasts are considered three separate data stories individually each of these data stories can be very computationally expensive to bring you a decade wrapped we had to process these data stories over years worth of data for all of our monthly active users mau of which we have over million therefore the biggest challenge we faced for wrapped was scale in the wrapped campaign data pipeline had one of the largest dataflow jobs to ever run on gcp google cloud platform which resulted in limits around the amount of data we were able to shuffle the amount of data we tried to process for just with basic pipeline design struggled to complete at the scale we were attempting due to the scope of the work we had to work closely with google cloud to implement the ability to handle the data at such a large scale from the prior wrapped team s experience we took learnings which heavily influenced our decisions in designing the architecture for the campaign architecture in wrapped the main challenges we faced with respect to the decade concept were getting the listening history and summarizing thousands of historical data points per user over years in a cost effective manner while ensuring data quality reading the data spotify has user listening analytics data dating back to our first years as a streaming platform but it is difficult to read this amount of data in a single job coupled with how we store user data with a focus on protecting user privacy there ends up being a potential of resource capacity issues we knew we wanted several separate statistics drawn from each year of the decade and that required having access to the user listening analytics data for each individual year fortunately we have a system developed internally to help us accomplish this in an efficient fashion by giving us access to time series data this data lake is backed by google cloud bigtable and is highly optimized for aggregating data over an arbitrary time range we were able to use this listening history analytics data source to get and summarize user stories for not only each year of the decade but also for targeted windows of each season of processing the data the biggest change in design from involved storing the data in a way that would reduce the amount of shuffles needed to group all the data stories on a user level this was especially important since in we were working with about x the amount of data over years to reduce the amount of shuffles we decided to use bigtable as our final data store because additional logic needed to be implemented on top of these data stories we chose to store the intermediate output in a separate bigtable instance and performed additional processing at the end before final storage having one row per user in bigtable and storing the output from different data story jobs to that same row but separate column families gave us the benefit of having that data pre grouped and colocated on a user level image the architecture for our data pipelines for wrapped as most of the data stories weren t dependent on each other we were able to write separate jobs to compute them and save the outputs to the same row in bigtable this allowed us to run and iterate on individual data story jobs in a parallel fashion saving us a lot of time this also prevented us from having to re run all jobs if a mistake was made or if there were any last minute requirement changes the exception would be calculating the statistics for the top of the decade as that required us to process all years of data together however since we already had metrics pre grouped and aggregated for each year in our bigtable we decided to go with the approach of reading and combining that data to get top of the decade metrics this not only saved us the time and cost of processing and aggregating all that data in one job but also made testing easier by giving us a means to pull a sub sample of the decade data data qa in order to iterate fast and to ensure the quality of our data we needed a tool to be able to access data directly from our intermediate and final storage to achieve this we wrote a python library that used cloud bigtable apis to read data from our bigtable having this resource not only enabled us to eyeball the data for sanity checks but also helped us catch and validate bugs quickly key takeaway s leverage the right system design and data store to reduce cost we processed x the amount of data compared to the wrapped campaign while spending less overall for processing this was achieved by designing our system in a way that reduced group by key operations and reused the output of yearly jobs to produce top of the decade metrics decouple data processes to improve iteration speed because we were able to break down the users summaries into smaller data stories and workflows we ended up with a much more flexible system that allowed for quick iteration and exploration therefore any last minute requirement changes were isolated to individual data stories and workflows and had no impact on the rest of the data stories acknowledgments the data engineering team called time turners for their ability to transport users back in time through their listening history was made up of a group of engineers from spotify s personalization mission the team included catie edwards data ml engineer zoe tiet data backend engineer bindia kalra data backend engineer erin palmer senior data engineer alissa deak data scientist owen heneghan staff engineer maya hristakeva engineering manager and marc tollin product manager a big shout out to this amazing team and our friends in data curation and t rex who worked tirelessly to bring you your decade wrapped',\n",
              " ' our massive data pipeline has helped us process enormous amounts of information over the past decade all to help us help our users discover read and learn in this blog series i will share how we re upgrading the pipeline to give internal customers faster and more reliable results the data pipeline is currently managed using a home grown workflow orchestration system written in ruby called datapipe the first commit of our data pipeline repo dates all the way back to we created it around the time when everybody else was building their own orchestration tools such as pinterest s pinball spotify s luigi or airbnb s airflow these tools all perform the same basic function process and execute a directed acyclic graph dag of work typically associated with a etl data pipelines today we have tasks and dags with the majority of tasks globbed together in one giant dag containing more than tasks datapipe has served us well and brought the company to where it is today however it has been in maintenance mode for some time as a result it s struggling to meet the needs of scribd s fast growing engineering team since scribd is moving more and more into the cloud we decided that now is a good time for us to step back and redesign the system for the future we need a modernized workflow orchestration system to help drastically improve productivity and unlock the capability to build new product features that were not previously possible opportunity for improvement here are some of the areas we think would result in big impacts to the organization flexibility the in house system can only setup run schedule at the granularity of one day which sets a limit on freshness of our data to unlock new applications we need to let engineers to define schedules with more flexibility and granularity productivity culturally we would like to shift from mono repo to multi repo needless to say putting all the workflow definitions in a single file is not scalable our workflow config today already contains lines of code and is still growing by building tooling to support multi repo setup we hope to reduce coupling and speed up development cycles ownership today we have dedicated engineers keeping eyes on nightly runs to notify workflow owners if anything goes wrong the web ui doesn t support some of the common maintenance actions like killing a running tasks this combined with lack of built in monitoring and alerting support within the orchestration system means even if workflow owners want to take full ownership of their tasks there is no easy way to accomplish it we need to flip this around and empower workflow owners to take care of their own tasks end to end this is the only scalable way going forward scalability and availability the orchestration system should be able to handle the scale of data pipeline for many years to come it should also be highly available and function without issue when a minority of the cluster goes down operability minor failures in the pipeline should not impact the rest of the pipeline recovering failed tasks should be easy and fully self serviced extensibility it s not surprising that after many years of development the in house system comes with many unique and useful features like cross date dependencies it should be easy to develop and maintain custom features for the new system cloud native as we migrate its infrastructure from a datacenter to the cloud the new system will need to be able to run smoothly in the cloud and integrate nicely with various software as a service offerings like datadog pagerduty and sentry we basically had two options retrofit datapipe or pick a well maintained open source project as the building block after lots of prototyping and careful evaluation we decided to adopt apache airflow super charging airflow i wish adopting airflow is just as simple as doing a pip install and pointing the config to a rds endpoint it turns out we had to do a lot of preparation work to make it meet all our requirements just to name a few implement scalable and highly available setup leveraging both ecs and eks write tooling to support defining dags in multiple repositories scale airflow to handle one of our gigantic dag create custom airflow plugins to replicate some of the unique features from the in house system build dag delivery pipeline with a focus on speed and separation of environments monitor airflow itself as well as dags and tasks with datadog pagerduty and sentry execute multi stage workflow migration from the in house system each one of the above items warrants a blog post of its own we will be sharing what we have learned in more detail throughout this series of blog posts at scribd we embrace open source and try to contribute back to the community as much as we can since the start of this internal project we have contributed more than patches upstream to airflow including eks support pagerduty hooks many bug fixes and performance improvements we hope to continue this trend and contribute more as the project progresses if this sounds interesting to you the core platform team is hiring come join us if you love building scalable data ml platforms using open source technologies',\n",
              " ' this blog post was originally published on the gitlab unfiltered blog it was reviewed and republished on from april to may i was the manager of the data team for gitlab i took this role after my manager left when i started reporting directly to the cfo as a data engineer i remember saying to him this doesn t seem like the right level of abstraction for you and proposed i step up to become the manager i also said i didn t want to do this for a long period of time since i intentionally came to gitlab to move from a manager role to an individual contributor role and focus on data engineering what follows are a few lessons i learned and relearned in my one year stint as the manager of the data team eventually i aim to become a manager again and i hope to remember these lessons and learn even more plan for growth while i was data team manager gitlab grew in size by having only worked previously at established companies and at a very small startup i was not prepared for this level of growth and the strain it would put on our resources i recently surveyed colleagues of mine in the data community and discovered that as a percentage of headcount most data teams are anywhere from this means a person company should have at least four people and realistically around people focused on data this includes analysts engineers scientists and managers in april of we were at and would continue to be throughout as the company grew i did not wholly understand how the business was planning to grow and how the data team would scale to meet the data needs of the organization this lack of strategic thinking led to a situation where i felt blindsided and overwhelmed by the number of requests for data and analytics even with the addition of the excellent people i was able to hire i wasn t doing as good a job as i needed to help my team truly succeed lesson understand the trajectory of the company the workload you have and expect to have pick a gearing ratio for headcount stick to your hiring targets and think about team structure individual contributor or manager pick one by the end of the data team was a three person team one data analyst one data engineer and me thankfully the three of us were i m not ashamed to say excellent at our jobs and performed at a level beyond what you would expect three ftes to handle but even we have limits and couldn t do it all due to the volume of work we were trying to accomplish it was critical that i take on analyst and engineering work as well this created a situation where i was splitting my brain and my attention trying to do too many things at once some days would be all manager work and i would make zero progress on issues assigned to me others would be ic work and i would fall behind on managerial tasks the worst days were when i would try to do both and everything would suffer as time went on this split brain effect would become worse the signs of burnout were starting to ramp up rapidly i was able to hire more people which put more demand on the manager side of me yet the volume of work was increasing while i was still the primary contributor and maintainer of our codebase by the end i didn t feel like i was a good manager and i felt like my technical skills were rapidly atrophying lesson if you re a manager be a manager yes you ll have to pick up some work especially at a startup but figure out your exit plan so you can pass that work to your team who will be much better at accomplishing it than you hire awesome people this should go without saying but hire excellent people and your life will be better my first four hires for the data team two in two in early have blown me away with their skill curiosity tenacity and intelligence i learned from my previous job and past bosses the value in finding great people and the force multiplier they can have on the work you re trying to accomplish lesson continue hiring great people but think about how to scale it invest in process this lesson i learned from emilie schario the first data analyst i hired she taught me to think about how and where we ll need processes as the company scaled so we could remain efficient we of course used gitlab for managing our code and we had built in merge request workflows but she took the time to think about the messy people stuff surrounding the technology a short list of artifacts she created and many more i m sure i m forgetting while she wasn t the manager she had the experience and understood the parts of working at a company that can slow down team members and she worked to automate as much of it as possible i ve heard from many people outside the company how much they appreciate our documentation in general and our onboarding process in particular that is a testament to thinking about scale and having the empathy to continually step into the shoes of a gitlab learner and to see things from an outsider s perspective as data teams have grown and evolved they ve also become more technical these mean it s important to invest in the technical process as well this means you should have version control change control merge requests automated testing and documentation on everything you re doing certain tools make implementing technical processes better and easier which i ll highlight in the next section lesson think about process deeply and document everything maintain the mind of a learner and continually think about what day one with gitlab is like for new people invest in process documentation and testing they are gifts you give your future self along with process picking the right tools can be a force multiplier for team productivity when the data team started we were using postgresql as our data warehouse postgres is not column oriented and at a certain point it doesn t make sense to use it as an analytics database we went with postgres anyway because we believe in using a boring solution and it aligns with our value of iteration for the volume of data we were throwing at it postgres did admirably we used the cloudsql hosted version which enabled us to do cool programmatic things with gitlab ci i ll save that for another post once we outgrew postgres we decided to move to snowflake of course being gitlab we use gitlab the product for anything and everything which saved us much of the stress around picking tools it has all the things you want from a coding perspective and it has enough of the things you need to be productive as a manager no need for trello jira and a dozen other tools by far though the best tool for the data team s productivity is dbt data build tool i could talk forever about how great dbt is but suffice to say that we would not be where we are today and we would not have been able to support the organization this well with such a small crew were it not for dbt and the great community behind it lesson find the best tools you can for your team use dbt handling under performers is a challenge up until i d never hired somebody who didn t perform well in their job aside from a few interns i d like to think most of this was my ability to find good people but it was probably luck if i m being honest last year challenged me with two under performers on the team that i now realize i could have supported better having those difficult conversations with people was hard when i wasn t in the manager brain space my advice is to pay attention to those first few weeks of productivity and if you find there are gaps either in skills or motivation do whatever you can to call out the gaps in a friendly and productive way and then give your people every opportunity to become better lesson be a good manger notice things early and help your team proactively so many meetings gitlab has a great culture around meetings they always start on time there must be an agenda for every meeting and people aren t afraid to end meetings early if everything on the agenda is done even with this rigor and discipline you will find yourself on the manager s schedule and will be in a lot of meetings but that s okay that s part of your job i will always argue that you should still try to reduce the time you re in meetings but if you re in a meeting do your best to ensure your team isn t also in a meeting if at all possible meetings are terrible for makers i e your direct reports shield your team from them as much as possible lesson meetings are a part of the job reduce them as much as you can and protect your team from unnecessary meetings you need executive buy in and representation part of the reason i was excited to join gitlab was because the c suite clearly supported having a data team in the organization the ceo and cfo understood the value a data team could bring even if the specifics and execution were blurry this is important you will be in a tough spot if your company has nobody on the executive team that understands the value that good descriptive and predictive analytics can provide data literacy is a cultural attribute and it s near impossible to grow literacy in an organization if the ceo isn t driving it in some way at a certain scale though you need data leadership beyond a team manager you absolutely need someone at the director level and up that can advocate and champion data literacy and fluency across the functional areas of the organization managers can t be expected to spend much time on this since there is so much daily work to be done lesson be wary of organizations that don t have c suite buy in around the data function advocate for a director level and up position that can be the cheerleader for data across the organization plan to spend some money executive level buy in for a data team is important because of this fact starting a data team can be expensive to be effective you ll need to hire several people or empower your single data lead to purchase some third party software out of the gate you ll need an extract and load tool like stitch or fivetran you ll need a data warehouse e g snowflake bigquery redshift you ll need compute to run transform jobs and you ll want a bi tool there are free tools that can sustain you for a while but plan to invest some money up front if you re in it for the long haul lesson long term success will require investment you can start cheaply but to scale requires resources don t reinvent the wheel especially for things like extracting data from tools such as salesforce zendesk or zuora please please please don t write your own scripts to do this just pay a company to do it for you you ll waste a ton of time doing something that doesn t deliver business value and will probably come back to bite you in the end you should spend most of your time delivering value for the business in the form of automated reporting and generating insights not writing a salesforce to snowflake extractor for the thousandth time lesson pay for stitch or fivetran for common data extractions manager is a different career don t think about becoming a manager as an extension of your individual contributor career it is a different career path and your ic skills will certainly help you be a better manager however management is its own set of skills and choosing to go into this field puts you on a different career path it s not necessarily better depending on how you define success go into management with open eyes and a full understanding that you are switching tracks and not moving ahead it isn t permanent though and can be reversed if you choose lesson don t assume the move to manager is the default for an ic think deeply about your career read about the engineer manager pendulum it s okay to be a little selfish one area i ve struggled with for a while is making the effort to be a little selfish i can have a people pleaser mentality which when applied to the business of a startup can be useful startups need people that are willing to do what it takes to make the company successful within reason but once the company is in a growth stage or beyond that mentality is a recipe for burnout at my previous company we were less than people having the attitude of trying to do and learn as much as possible was a good strategy for me i learned a ton was given a bunch of responsibility and helped the business grow that strategy worked for me at gitlab for a while too after some time passed it was clear i couldn t keep up with everything and my sanity would start to suffer without a fix being selfish in this case meant i had to be okay with wanting to take a step back from the manager role to the ic role spoiler it s not a step back see the previous point i had to admit to myself that i wanted to focus on programming more and that continuing down the manager track wasn t currently right for me it felt selfish because it was hard in the moment to see that what the business needed was somebody who wanted to be the manager it didn t need me to continue in the role just because i happened to currently be in the role while there were short term ramifications for the team because of my move to an ic role i know that i m healthier for it and we now have two excellent managers who are leading the team further than i could have lesson it s a good thing to prioritize and be selfish about your mental health it s okay to say no i can t do this anymore companies need people who want to be in their jobs performance is better and people are happier fin my hope is that these lessons are valuable to you and are applicable in your own life and career i would love to hear from you if you disagree with any of these or if you have your own stories and lessons to share about your career in data please reach out on twitter via email tmurphy at gitlab com or in an issue in our main project thank you for reading and thank you to gitlab for enabling my growth as a data professional special thanks to emilie schario for her review on multiple drafts of this post',\n",
              " ' envoy provides kafka broker level filter that allows us to collect the request response metric for a given broker the filter decodes the received requests responses and updates the correct metrics this way we can find out how many requests were received by given broker how many responses were sent and how much time was spent for processing in this short article i will be covering two ways how envoy kafka can be deployed routing all kafka related traffic through envoy including internal cluster communication routing only kafka client traffic through envoy in the examples i will be using very simple local deployment kafka brokers listening on and envoy proxy instance that proxies them on ports to avoid a single point of failure in production scenario it might be preferable to have one envoy instance per kafka broker routing all traffic through envoy including replication on envoy side all we need to do is to route incoming traffic to correct broker ports we are going to route all traffic coming to to and to kafka clients that would want to connect to the cluster should use bootstrap servers ocalhost localhost to establish initial connections configuration for envoy the server configuration is really simple the only trick here is that our kafka servers need to advertise envoy s ports advertised listeners as it s the only value that will get published to zookeeper everything both kafka clients and other kafka brokers will need to go through envoy to reach our broker configuration for kafka broker configuration for kafka broker when we get everything running we can see that some internal traffic has been collected by our metrics collecting filter kafka broker request update_metadata_request kafka broker response update_metadata_response kafka broker request update_metadata_request kafka broker response update_metadata_response after we create a topic e g via kafka topics tool we can see that replication is happening the fetch_request fetch_response metrics increase even if there are no clients this is caused by broker replicator threads that periodically sending fetch requests to partition leaders routing only client traffic through envoy as an alternative we might want to have only kafka client traffic routed through envoy while the internal cluster communication should happen without any proxying our envoy broker configuration is not changing the clients still want to reach out to ports and they will be routed as usual on broker side we need to make our brokers listen on new replication port we will use and make sure that brokers know which address should they use for replication with inter broker listener name configuration for kafka broker configuration for kafka broker after starting all the services but before starting any of the clients we can see that kafka metrics are not present as envoy is not involved in the traffic only after we decide to connect to kafka with some kind of client we can see something happening let s use kafka console consumer create topic mytesttopic and make sure it is replicated bin kafka topics sh bootstrap server localhost localhost create topic mytesttopic replication factor partitions start the consumer bin kafka console consumer sh bootstrap server localhost localhost topic mytesttopic when the consumer is active what internally means sending fetchrequest objects in a loop we can see the metrics increase in envoy caused by kafka topics invocation kafka broker request create_topics_request constantly increasing while consumer is alive kafka broker request fetch_request kafka broker response fetch_response kafka broker request fetch_request kafka broker response fetch_response',\n",
              " ' wherehows is now datahub linkedin s metadata team has previously introduced datahub successor of wherehows linkedin s metadata search and discovery platform and shared plans to open source it shortly after that announcement we released an alpha version of datahub and shared it with the community since then we have continuously contributed to the repo and worked with interested users to add most requested features and resolve issues now we are proud to announce the official release of datahub on github open source approaches wherehows linkedin s original data discovery and lineage portal started as an internal project the metadata team open sourced it in from that time onwards the team has always maintained two different codebases one for open source and the other for linkedin s internal use because not all product features developed for linkedin s use cases were generally applicable to a broader audience also wherehows had some internal dependencies infrastructure libraries etc which are not open sourced wherehows went through a lot of iterations and development cycles in the following years which made keeping the two codebases in sync a big challenge the metadata team attempted different approaches over the years to try to make internal and open source development in sync with each other first attempt open source first initially we followed an open source first development model where the main development takes place in the open source repo and changes are pulled in for internal deployment the problem with this approach is that the code is always pushed to github first before it is fully validated internally until the changes from the open source repo were pulled in and a new internal deployment took place we would not discover any production issues in the case of a bad deployment it was also very hard to figure out the culprit because changes were pulled in batches also this model decreased the productivity of the team when developing new features that needed fast iterations because it forced all changes to be pushed to the open source repo first and then brought them to the internal repository to reduce turnaround time the necessary fix or change could be done first in the internal repository but this became a huge pain point when it came to merging those changes back to the open source repo because the two repositories had gotten out of sync this model is much easier to implement for generic frameworks libraries or infrastructure projects than it is for full stack custom web applications also this model is perfect for projects that start out as open source from day one but wherehows had started out as a completely internal web app it was really difficult to cleanly abstract all internal dependencies which is why we needed to keep an internal fork but keeping an internal fork and developing primarily in open source did not quite work for us second attempt internal first as a second attempt we switched to an internal first development model where the main development takes place internally and changes are pushed to open source on a regular basis although this model is best suited for our use case it has inherent challenges directly pushing all the diff to the open source repo and then trying to resolve merge conflicts later is an option but it s time consuming developers will mostly avoid doing it with every code check in as a result it will be done much less frequently in batches and thus increases the pain of resolving merge conflicts later third time s the charm the two failed attempts mentioned above had the consequence of leaving the wherehows github repository stale for a long time the team continued to iterate on the product features and architecture so linkedin s internal version of wherehows quickly became a better and much improved version than the open source one it even had a new name datahub learning from previous failed attempts the team has decided to devise a scalable long term solution for any new open source project linkedin s open source team advises and supports a development model where building blocks modules of the project are fully developed in open source versioned artifacts are deployed to a public repository and then brought back to linkedin s internal artifactory using external library request elr following this development model is not only good for the open source community but also results in a more modular extensible and pluggable architecture however to achieve that state for a mature internal application like datahub will take a significant amount of time it also precludes the possibility of open sourcing a fully working implementation before all internal dependencies are completely abstracted out therefore we ve developed tooling that helps us make open source contributions faster and much less painful in the interim this is a decision benefiting both the metadata team the developer of datahub and the open source community the following sections will discuss this new approach automating open source contributions the metadata team s latest approach for open sourcing datahub is to develop a tool that automatically syncs the internal codebase and the open source repository high level features of this tooling include syncing of linkedin code to from open source similar to rsync license header generation similar to apache rat auto generation of open source commit logs from internal commit logs preventing internal changes that break open source build via dependency testing in the following subsections the above features which have interesting challenges will be discussed in detail source code syncing as opposed to the open source version of datahub which is a single github repo linkedin s version of datahub is a combination of multiple repos known internally as multiproducts datahub s frontend metadata models library metadata store backend service and streaming jobs sit in different repositories within linkedin however for an easier experience for open source users we have a single repository for the open source version of datahub',\n",
              " ' feb seth wiesman sjwiesman introduction the recent apache flink release includes many exciting features in particular it marks the end of the community s year long effort to merge in the blink sql contribution from alibaba the reason the community chose to spend so much time on the contribution is that sql works it allows flink to offer a truly unified interface over batch and streaming and makes stream processing accessible to a broad audience of developers and analysts best of all flink sql is ansi sql compliant which means if you ve ever used a database in the past you already know it a lot of work focused on improving runtime performance and progressively extending its coverage of the sql standard flink now supports the full tpc ds query set for batch queries reflecting the readiness of its sql engine to address the needs of modern data warehouse like workloads its streaming sql supports an almost equal set of features those that are well defined on a streaming runtime including complex joins and match_recognize as important as this work is the community also strives to make these features generally accessible to the broadest audience possible that is why the flink community is excited in to offer production ready ddl syntax e g create table drop table and a refactored catalog interface accessing your data where it lives flink does not store data at rest it is a compute engine and requires other systems to consume input from and write its output those that have used flink s datastream api in the past will be familiar with connectors that allow for interacting with external systems flink has a vast connector ecosystem that includes all major message queues filesystems and databases if your favorite system does not have a connector maintained in the central apache flink repository check out the flink packages website which has a growing number of community maintained components while these connectors are battle tested and production ready they are written in java and configured in code which means they are not amenable to pure sql or table applications for a holistic sql experience not only queries need to be written in sql but also table definitions create table statements while flink sql has long provided table abstractions atop some of flink s most popular connectors configurations were not always so straightforward beginning in flink supports defining tables through create table statements with this feature users can now create logical tables backed by various external systems in pure sql by defining tables in sql developers can write queries against logical schemas that are abstracted away from the underlying physical data store coupled with flink sql s unified approach to batch and stream processing flink provides a straight line from discovery to production users can define tables over static data sets anything from a local csv file to a full fledged data lake or even hive leveraging flink s efficient batch processing capabilities they can perform ad hoc queries searching for exciting insights once something interesting is identified businesses can gain real time and continuous insights by merely altering the table so that it is powered by a message queue such as kafka because flink guarantees sql queries have unified semantics over batch and streaming users can be confident that redeploying this query as a continuous streaming application over a message queue will output identical results define a table called orders that is backed by a kafka topic the definition includes all relevant kafka properties the underlying format json and even defines a watermarking algorithm based on one of the fields so that this table can be used with event time create table orders user_id bigint product string order_time timestamp watermark for order_time as order_time seconds with connector type kafka connector version universal connector topic orders connector startup mode earliest offset connector properties bootstrap servers localhost format type json define a table called product_analysis on top of elasticsearch where we can write the results of our query create table product_analysis product string tracking_time timestamp units_sold bigint with connector type elasticsearch connector version connector hosts localhost connector index productanalysis connector document type analysis a simple query that analyzes order data from kafka and writes results into elasticsearch insert into product_analysis select product_id tumble_start order_time interval day as tracking_time count as units_sold from orders group by product_id tumble order_time interval day catalogs while being able to create tables is important it often isn t enough a business analyst for example shouldn t have to know what properties to set for kafka or even have to know what the underlying data source is to be able to write a query to solve this problem flink also ships with a revamped catalog system for managing metadata about tables and user definined functions with catalogs users can create tables once and reuse them across jobs and sessions now the team managing a data set can create a table and immediately make it accessible to other groups within their organization the most notable catalog that flink integrates with today is hive metastore the hive catalog allows flink to fully interoperate with hive and serve as a more efficient query engine flink supports reading and writing hive tables using hive udfs and even leveraging hive s metastore catalog to persist flink specific metadata looking ahead flink sql has made enormous strides to democratize stream processing and marks a significant milestone in that development however we are not ones to rest on our laurels and the community is committed to raising the bar on standards while lowering the barriers to entry the community is looking to add more catalogs such as jdbc and apache pulsar we encourage you to sign up for the mailing list and stay on top of the announcements and new features in upcoming releases',\n",
              " ' extending relational query processing with ml inference karanasos cidr this paper provides a little more detail on the concrete work that microsoft is doing to embed machine learning inference inside an rdbms as part of their vision for enterprise grade machine learning the motivation is not that inference will perform better inside the database but that the database is the best place to take advantage of enterprise features transactions security auditing ha and so on given the desire to keep enterprise data within the database and to treat models as data also the question is can we do inference in the database with acceptable performance raven is the system that microsoft built to explore this question and answer it with a resounding yes based on interactions with enterprise customers we expect that storage and inference of ml models will be subject to the same scrutiny and performance requirements of sensitive mission critical operational data when it comes to data database management systems dbmss have been the trusted repositories for the enterprise we thus propose to store and serve ml models from within the dbms the authors don t just mean farming inference out to an external process from within the rdbms but deeply integrating ml scoring as an extension of relational algebra and an integral part of sql query optimisers and runtimes the vision is that data scientists use their favourite ml framework to construct a model which together with any data pre processing steps and library dependencies forms a model pipeline in raven model pipelines are packaged using mlflow these pipelines are stored directly in the database a stored model pipeline can then be invoked a bit like a stored procedure by issuing sql commands end to end the picture looks like this enlarge an insert into model statement adds the source code for the model pipeline python in the example to the database at some later point a sql query is issued which select s a model and then uses the predict function to generate a prediction from the model given some input data which is itself of course the result of a query the combined model and query undergo static analysis to produce an intermediate representation ir of the prediction computation as a dag a cross optimiser then looks for opportunities to optimise the data operator parts of the query given the ml model and vice versa e g pruning a runtime code generator creates a sql query incorporating all of these optimisations an extended version of sql server with an integrated onnx runtime engine executes the query static analysis and the ir in an attempt to be as accommodating as possible to existing data science workflows the models in model pipelines are simply expressed in python in mlflow open model format unlike sql queries which are declarative such models are expressed as imperative programs heavily dependent on libraries the static analyser has built in knowledge of popular frameworks and libraries and knows how to map dataflow nodes and subgraphs to ir operators there are limitations to this model of course it can t handle loops for example an analysis of m python notebooks in github showed that of notebook used loops conditionals result in one plan per execution path and dynamic typing means not all types can be inferred statically using knowledge from the sql queries that feed data into the models for inference can help here if the static analyser runs into something it doesn t know how to translate into ir it translates the offending bit of code into a udf user defined function instead the ir itself contains all the relational algebra operators you would expect in an rdbms together with a collection of linear algebra operators e g matrix multiplication and convolution higher level machine learning operators e g decision trees and data featurizer operators e g categorical encoding maybe in time there will be a more declarative way of specifying the model rather than trying to recover it from an imperative program cross optimisation by using state of the art relational and ml engines raven can also leverage the large body of work in relational and ml inference optimization beyond the standard optimisations that exist solely in the relational or solely in the ml realm raven also benefits heavily from cross ir optimisations that pass information between data and ml operators in predicate based model pruning knowledge about the data flowing into the model e g due to constraints in a where clause can be used to prune the execution paths in the model for example removing a branch from a decision tree that we know can never be taken in model projection pushdown we go the other way and use knowledge about the model e g that certain features do not contribute to the prediction output to optimise the data processing part of an inference query by fetching only the data that will actually be used to create more opportunities for predicate based pruning we can cluster data so that each cluster has specific values for some features and then create one optimised model per cluster model clustering model inlining optimisations replace ml operators with relational ones enabling use of the high performance sql query optimiser and execution engine nn translation optimisations replace classical ml operators and data featurizers with neural networks that can be executed directly in e g onnx runtime pytorch or tensorflow this is very important performance wise unlike most traditional ml frameworks nn engines support out of the box hardware acceleration through gpus fpgas npus as well as code generation query execution for models that are fully supported by the static analyser raven supports in process execution using the embedded onnx runtime onnx runtime is used as a dynamically linked library to create inference sessions transform data to tensors and invoke in process predictions over any onnx model or any model that can be expressed in onnx through raven s static analysis or onnx converters for model pipelines that are written in python or r but not supported by the static analyser sql server s external script support is used for out of process execution for model pipelines in an unsupported language raven falls back to containerised execution the following chart shows the comparative performance of raven using the in process onnx runtime execution raven against raven with external execution raven ext and against a pure onnx runtime ort for a random forest and an mlp based model the dataset size here refers to the number of tuples for which predictions are to be made not the size of training data so we re essentially evaluating batch prediction the chart seems to show raven outperforming at nearly all scales there s a band in the middle between k k tuples where raven is up to slower than ort but as we scale up it is x faster with m to m tuples this speed up came through sql server automatically parallelising the scan and predict operators as compared to the ort sequential execution for single or very small numbers of predictions raven is faster due to sql server s caching the last word we are busy incorporating the techniques we present in this paper in a full fledged cost based optimiser hardware acceleration and multi query optimisation will make this even more fun if you want to play with these ideas for yourself raven s sql server with integrated onnx runtime is currently in public preview in azure s sql database edge',\n",
              " ' if you are here i do not need to tell you that elasticsearch is awesome fast and mostly just works if you are here i also do not need to tell you that elasticsearch can be opaque confusing and seems to break randomly for no reason in this post i want to share my experiences and tips on how to set up elasticsearch correctly and avoid common pitfalls i am not here to make money so i will mostly just jam everything into one post instead of doing a series feel free to skip sections the basics clusters nodes indices and shards if you are really new to elasticsearch es i want to explain some basic concepts first this section will not explain best practices at all and focuses mainly on explaining the nomenclature most people can probably skip this elasticsearch is a management framework for running distributed installations of apache lucene a java based search engine lucene is what actually holds the data and does all the indexing and searching es sits on top of this and allows you to run potentially many thousands of lucene instances in parallel the highest level unit of es is the cluster a cluster is a collection of es nodes and indices nodes are instances of es these can be individual servers or just es processes running on a server servers and nodes are not the same a vm or physical server can hold many es processes each of which will be a node nodes can join exactly one cluster there are different types of node the two most interesting of which are the data node and the master eligible node a single node can be of multiple types at the same time data nodes run all data operations that is storing indexing and searching of data master eligible nodes vote for a master that runs the cluster and index management indices are the high level abstraction of your data indices do not hold data themselves they are just another abstraction for the thing that actually holds data any action you do on data such as inserts deletes indexing and searching run against an index indices can belong to exactly one cluster and are comprised of shards shards are instances of apache lucene a shard can hold many documents shards are what does the actual data storage indexing and searching a shard belongs to exactly one node and index there are two types of shards primary and replica these are mostly the exact same they hold the same data and searches run against all shards in parallel of all the shards that hold the same data one is the primary shard this is the only shard that can accept indexing requests should the node that the primary shard resides on die a replica shard will take over and become the primary then es will create a new replica shard and copy the data over at the end of the day we end up with something like this a more in depth look at elasticsearch if you want to run a system it is my belief that you need to understand the system in this section i will explain the parts of elasticsearch i belief you should understand if you want to manage it in production this will not have any recommendations in it those come later instead it aims purely at explaining necessary background quorum it is very important to understand that elasticsearch is a flawed democracy nodes vote on who should lead them the master the master runs a lot of cluster management processes and has the last say in many matters es is a flawed democracy because only a subclass of citizens the master eligible nodes are allowed to vote master eligible are all nodes that have this in their configuration node master true on cluster start or when the master leaves the cluster all master eligible nodes start an election for the new master for this to work you need to have n master eligible nodes otherwise it is possible to have a split brain scenario with two nodes receiving of the votes this is a split brain scenario and will lead to the loss of all data in one of the two partitions so don t have this happen you need n master eligible nodes how nodes join the cluster when an es process starts it is alone in the big wide world how does it know what cluster it belongs to there are different ways this can be done however these days the way it should be done is using what is called seed hosts basically elasticsearch nodes talk with each other constantly about all the other nodes they have seen because of this a node only needs to know a couple other nodes initially to learn about the whole cluster edit th feb dave turner mentioned in the comments this isn t really a constant process nodes only share information about other nodes they have discovered when they re not part of a cluster once they ve joined a cluster they stop this and rely on the cluster s elected master to share any changes as they occur which saves a bunch of unnecessary network chatter also in x they only really talk about the master eligible nodes they have seen the discovery process ignores all the master ineligible nodes davecturner lets look at this example of a three node cluster initial state in the beginning node a and c just know b b is the seed host seed hosts are either given to es in the form of a config file or they are put directly into elasticsearch yml node a connects and exchanges information with b as soon as node a connects to b b now knows of the existence of a for a nothing changes node c connects and shares information with b now c connects as soon as this happens b tells c about the existence of a c and b now know all nodes in the cluster as soon as a connects to b again it will also learn of the existence of c segments and segment merging above i said that shards store data this is only partially true at the end of the day your data is stored on a file system in the form of files in lucene and with that also elasticsearch these files are called segments a shard will have between one and multiple thousand segments again a segment is an actual real file you can look at in the data directory of your elasticsearch installation this means that using a segment is overhead if you want to look into one you have to find and open it that means if you have to open many files there will be a lot of overhead the problem is that segments in lucene are immutable that is fancy language for saying they are only written once and can not be changed this in turn means that every document you put into es will create a segment with only that single document in it so clearly a cluster that has a billion documents has a billion segments which means there are a literal billion files on the file system right well no in the background lucene does constant segment merging it can not change segments but it can create new ones with the data of two smaller segments this way lucene constantly tries to keep the number of segments which means the number of files which means the overhead small it is possible to force this process by using a force merge message routing in elasticsearch you can run any command against any node in a cluster and the result will be the same that is interesting because at the end of the day a document will live in only one primary shard and its replicas and es does not know where there is no mapping saying a specific document lives in a specific shard if you are searching then the es node that gets the request will broadcast it to all shards in the index this means primary and replica these shards then look into all their segments for that document if your are inserting then the es node will randomly select a primary shard and put the document in there it is then written to that primary shard and all of its replicas edit th feb dave turner mentioned in the comments the word shard is ambiguous here and although i think you understand this correctly it d be easy for a reader to misinterpret how this is written each search only fans out to one copy of each shard in the index either a primary or a replica but not both if that fails then the search will of course try again on another copy but that s normally very rare davecturner so how do i run elasticsearch in production finally the practical part i should mention that i managed es mostly for logging i will try to keep this bias out of this section but will ultimately fail sizing the first question you need to ask and subsequently answer yourself is about sizing what size of es cluster do you actually need ram i am talking about ram first because your ram will limit all other resources heap es is written in java java uses a heap you can think of this as java reserved memory there is all kind of stuff that is important about heap which would triple this document in size so i will get down to the most important part which is heap size use as much as possible but no more than g of heap size here is a dirty secret many people don t know about heap every object in the heap needs a unique address an object pointer this address is of fixed length which means that the amount of objects you can address is limited the short version of why this matters is that at a certain point java will start using compressed object pointers instead of uncompressed ones that means that every memory access will have additional steps involved and be much slower you do not want to get over this threshold which is somewhere around g i once spend an entire week locked into a dark room doing nothing else but using esrally to benchmark different file systems heap sizes fs and bios settting combinations of elasticsearch long story short here is what it had to say about heap size index append latency lower is better the naming convention is fs_heapsize_biosflags as you can see starting at g of heap size performance suddenly starts getting worse same with throughput index append median throughput higher is better long story short use g of ram or if you are feeling lucky use xfs and use hardwareprefetch and llc prefetch if possible fs cache most people run elasticsearch on linux and linux uses ram as file system cache a common recommendation is to use g for your es servers with the idea that it will be half cache half heap i have not tested fs cache however it is not hard to see that large es clusters like for logging can benefit greatly from having a big fs cache if all your indices fit in heap not so much edit th feb dave turner mentioned in the comments elasticsearch x uses a reasonable amount of direct memory on top of its heap and there are other overheads too which is why the recommendation is a heap size no more than of your physical ram this is an upper bound rather than a target a gb heap on a gb host may not leave very much space for the filesystem cache filesystem cache is pretty key to elasticsearch lucene performance and smaller heaps can sometimes yield better performance they leave more space for the filesystem cache and can be cheaper to gc too davecturner cpu this depends on what you are doing with your cluster if you do a lot of indexing you need more and faster cpus than if you just do logging for logging i found cores to be more than sufficient but you will find people out there using way more since their use case can benefit from it disk not as straightforward as you might think first of all if your indices fit into ram your disk only matters when the node is cold secondly the amount of data you can actually store depends on your index layout every shard is a lucene instance and they all have memory requirement that means there is a maximum number of shards you can fit into your heap i will talk more about this in the index layout section generally you can put all your data disks into a raid you should replicate on elasticsearch level so losing a node should not matter do not use lvm with multiple disks as that will write only to one disk at a time not giving you the benefit of multiple disks at all regarding file system and raid settings i have found the following things scheduler cfq and deadline outperform noop kyber might be good if you have nvme but i have not tested it queuedepth as high as possible readahead yes please raid chunk size no impact fs block size no impact fs type xfs ext index layout this highly depends on your use case i can only talk from a logging background specifically using graylog shards short version for write heavy workloads primary shards number of nodes for read heavy workloads primary shards replication number of nodes more replicas higher search performance here is the thing if you write stuff the maximum write performance you can get is given by this equation node_throughput number_of_primary_shards the reason is very simple if you have only one primary shard then you can write data only as quickly as one node can write it because a shard only ever lives on one node if you really wanted to optimize write performance you should make sure that every node only has exactly one shard on it primary or replica since replicas obviously get the same writes as the primary and writes are largely dependent on disk io note if you have a lot of indexing this might not be true and the bottleneck could be something else if you want to optimize search performance search performance is given by this equation node_throughput number_of_primary_shards number_of_replicas for searching primary and replica shards are basically identical so if you want to increase search performance you can just increase the number of replicas which can be done on the fly size much has been written about index size here is what i found g of heap shards maximum per node using more than shards i had elasticsearch processes crash with out of memory errors this is because every shard is a lucene instance and every instance requires a certain amount of memory that means there is a limit for how many shards you can have per node if you have the amount of nodes shards and index size here is how many indices you can fit number_of_indices number_of_nodes number_of_primary_shards replication_factor from that and your disk size you can easily calculate how big the indices have to be index_size number_of_nodes disk_size number_of_indices however keep in mind that bigger indices are also slower for logging it is fine to a degree but for really search heavy applications you should size more towards the amount of ram you have segment merging remember that every segment is an actual file on the file system more segments more overhead in reading basically for every search query it goes to all the shards in the index and from there to all the segments in the shards having many segments drastically increases read iops of your cluster up to the point of it becoming unusable because of this it s a good idea to keep the number of segments as low as possible there is a force_merge api that allows you to merge segments down to a certain number like if you do index rotation for example because you use elasticsearch for logging it is a good idea to do regular force merges when the cluster is not in use force merging takes a lot of resources and will slow your cluster down significantly because of this it is a good idea to not let for example graylog do it for you but do it yourself when the cluster is used less you definitely want to do this if you have many indices though otherwise your cluster will slowly crawl to a halt cluster layout for everything but the smallest setups it is a good idea to use dedicated master eligible nodes the main reasons is that you should always have n master eligible nodes to ensure quorum but for data nodes you just want to be able to add a new one at any time without having to worry about this requirement also you don t want high load on the data nodes to impact your master nodes finally master nodes are ideal candidates for seed nodes remember that seed nodes are the easiest way you can do node discovery in elasticsearch since your master nodes will seldomly change they are the best choice for this as they most likely already know all other nodes in the cluster edit th feb dave turner mentioned in the comments master eligible nodes are the only possible candidates for seed nodes in x because master ineligible nodes are ignored during discovery davecturner master nodes can be pretty small one core and maybe g of ram is enough for most clusters as always keep an eye on actual usage and adjust accordingly monitoring i love monitoring and i love monitoring elasticsearch es gives you an absolute ton of metrics and it gives you all of them in the form of json which makes it very easy to pass into monitoring tools here are some helpful things to monitor number of segments heap usage heap gc time avg search index merge time iops disk utilization conclusion after around hours of writing this i think i dumped everything important about es that is in my brain into this post i hope it saves you many of the headaches i had to endure resources https www elastic co guide en elasticsearch reference current modules node html https www elastic co guide en elasticsearch reference master modules discovery quorums html https github com elastic rally https tech ebayinc com engineering elasticsearch performance tuning practice at ebay',\n",
              " ' if you re a web developer check the instructions on how to implement the noscript code on your internet pages instructions for web developers you may want to consider linking to this site to educate any script disabled users on how to enable javascript in five most commonly used browsers you are free to use the code below and modify it according to your needs noscript for full functionality of this site it is necessary to enable javascript here are the a href https www enable javascript com instructions how to enable javascript in your web browser a noscript the instructions for your browser are put at the top of the page all the images are inlined full size for easy perusing this developer centric message is out of the way on enable javascript com we optimize the script disabled user experience as much as we can we want your visitors to have javascript enabled just as much as you do']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dok0fXJLqrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "Score_blog=[]\n",
        "index=[]\n",
        "for blog in cleaned_blog:\n",
        "  t=[]\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectors = vectorizer.fit_transform(blog)\n",
        "  feature_names = vectorizer.get_feature_names()\n",
        "  dense = vectors.todense()\n",
        "  denselist = dense.tolist()\n",
        "  df = pd.DataFrame(denselist, columns=feature_names)\n",
        "  df['mean'] = df.mean(axis=1)\n",
        "  Score_blog.append(max(df['mean']))\n",
        "  index.append(np.argsort(list(df['mean']))[-2:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XS5OOQW8QLI",
        "colab_type": "code",
        "outputId": "781e31b0-5710-4283-9d10-c04d6d1925f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "links[1]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://support.substack.com/hc/en-us?s=support%2Bdataengweekly%40substack.com',\n",
              " 'https://dataengweekly.substack.com/p/data-eng-weekly-356',\n",
              " 'https://slack.engineering/reliably-upgrading-apache-airflow-at-slacks-scale-2a31f3d03a06',\n",
              " 'https://labs.spotify.com/2020/02/18/wrapping-up-the-decade-a-data-story/',\n",
              " 'https://tech.scribd.com/blog/2020/modernizing-an-old-data-pipeline.html',\n",
              " 'https://about.gitlab.com/blog/2020/02/10/lessons-learned-as-data-team-manager/',\n",
              " 'https://medium.com/@adam.kotwasinski/deploying-envoy-and-kafka-8aa7513ec0a0',\n",
              " 'https://engineering.linkedin.com/blog/2020/open-sourcing-datahub--linkedins-metadata-search-and-discovery-p',\n",
              " 'https://flink.apache.org/news/2020/02/20/ddl.html',\n",
              " 'https://blog.acolyer.org/2020/02/21/extending-relational-query-processing/',\n",
              " 'https://facinating.tech/2020/02/22/in-depth-guide-to-running-elasticsearch-in-production/',\n",
              " 'https://substack.com/about?utm_source=substack&utm_medium=web&utm_content=footer',\n",
              " 'https://enable-javascript.com/']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClhauMT5Mh03",
        "colab_type": "code",
        "outputId": "d29858b4-c233-4fd4-8ad1-a113affdccf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "index"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 7]), array([9, 4]), array([2, 0]), array([4, 1])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HlWjKBN1hsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Selected_Text=[]\n",
        "Selected_link=[]\n",
        "Selected_title=[]\n",
        "Selected_keywords=[]\n",
        "Selected_date=[]\n",
        "Selected_Source=[]\n",
        "Selected_Summary=[]\n",
        "c=0\n",
        "for i in ((index)):\n",
        "  for j in i: \n",
        "    Selected_Text.append(Text[c][j])\n",
        "    Selected_link.append(link[c][j])\n",
        "    Selected_title.append(title[c][j])\n",
        "    Selected_keywords.append(keywords[c][j])\n",
        "    Selected_date.append(date[c][j])\n",
        "    Selected_Source.append(Source[c][j])\n",
        "#    Selected_Summary.append(summary[c][j])\n",
        "  c=c+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTUibGbfFigd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Kmeans_summary(text_):\n",
        "\n",
        "  from sklearn.feature_extraction import text\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "  from sklearn.cluster import KMeans\n",
        "\n",
        "  punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
        "  stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
        "\n",
        "  Kmeans_summary=[]\n",
        "  import numpy as np\n",
        "  from sklearn.metrics import pairwise_distances_argmin_min\n",
        "  from nltk.tokenize import sent_tokenize\n",
        "  sentences=sent_tokenize(text_)\n",
        "  vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
        "  X = vectorizer.fit_transform(sentences)\n",
        "  n_clusters=int(np.floor(len(sentences)**0.5))\n",
        "  print(n_clusters)\n",
        "  kmeans = KMeans(n_clusters=n_clusters,init='k-means++',max_iter=300,n_init=10,random_state=0)\n",
        "  kmeans.fit(X)\n",
        "  avg = []\n",
        "  for j in range(1):\n",
        "    idx = np.where(kmeans.labels_ == j)[0]\n",
        "    avg.append(np.mean(idx))\n",
        "  closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,X)\n",
        "  ordering = sorted(range(1), key=lambda k: avg[k])\n",
        "  Kmeans_summary=('.'.join([sentences[closest[idx]] for idx in ordering]))  \n",
        "\n",
        "  return Kmeans_summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSzR4PILFveP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Textrank_Summary(text):\n",
        "  from gensim.summarization import summarize\n",
        "  text= '\\n'.join(text.split(\".\"))\n",
        "  summary=(summarize(text))\n",
        "\n",
        "  return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iH8PWNXGUS-",
        "colab_type": "code",
        "outputId": "a5cdbd54-3041-4a85-af11-ee433534477a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "def LSA_Summary(url):\n",
        "  \n",
        "\n",
        "  from sumy.parsers.html import HtmlParser\n",
        "  from sumy.parsers.plaintext import PlaintextParser\n",
        "  from sumy.nlp.tokenizers import Tokenizer\n",
        "  from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
        "  from sumy.nlp.stemmers import Stemmer\n",
        "  from sumy.utils import get_stop_words\n",
        "\n",
        "\n",
        "  LANGUAGE = \"english\"\n",
        "  SENTENCES_COUNT = 4\n",
        "\n",
        "  parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
        "    \n",
        "  stemmer = Stemmer(LANGUAGE)\n",
        "\n",
        "  summarizer = Summarizer(stemmer)\n",
        "  summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "  s=\"\"\n",
        "  for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
        "    s=s+str(sentence)\n",
        "  Lsa=s\n",
        "    \n",
        "  return Lsa"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KUcqeoqG54C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Lexrank_Summary(url):\n",
        "  from sumy.parsers.html import HtmlParser\n",
        "  from sumy.parsers.plaintext import PlaintextParser\n",
        "  from sumy.nlp.tokenizers import Tokenizer\n",
        "  from sumy.summarizers.lex_rank import LexRankSummarizer as Summarizer\n",
        "  from sumy.nlp.stemmers import Stemmer\n",
        "  from sumy.utils import get_stop_words\n",
        "\n",
        "\n",
        "  LANGUAGE = \"english\"\n",
        "  SENTENCES_COUNT = 10\n",
        "\n",
        "  parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
        "  # or for plain text files\n",
        "  # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
        "  stemmer = Stemmer(LANGUAGE)\n",
        "\n",
        "  summarizer = Summarizer(stemmer)\n",
        "  summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "  s=\"\"\n",
        "  for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
        "    s=s+str(sentence)\n",
        "  Lex=s\n",
        "  \n",
        "  return Lex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbGRcp4bHQLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Reduction_Summary(url):\n",
        "  from sumy.parsers.html import HtmlParser\n",
        "  from sumy.parsers.plaintext import PlaintextParser\n",
        "  from sumy.nlp.tokenizers import Tokenizer\n",
        "  from sumy.summarizers.reduction import ReductionSummarizer as Summarizer\n",
        "  from sumy.nlp.stemmers import Stemmer\n",
        "  from sumy.utils import get_stop_words\n",
        "\n",
        "\n",
        "  LANGUAGE = \"english\"\n",
        "  SENTENCES_COUNT = 10\n",
        "\n",
        "  parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
        "  # or for plain text files\n",
        "  # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
        "  stemmer = Stemmer(LANGUAGE)\n",
        "  summarizer = Summarizer(stemmer)\n",
        "  summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "  s=\"\"\n",
        "  for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
        "     s=s+str(sentence)\n",
        "  reduction=s\n",
        "    \n",
        "  return reduction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BrJN8YkHr2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Luhn_summary(url):\n",
        "  from sumy.parsers.html import HtmlParser\n",
        "  from sumy.parsers.plaintext import PlaintextParser\n",
        "  from sumy.nlp.tokenizers import Tokenizer\n",
        "  from sumy.summarizers.luhn import LuhnSummarizer as Summarizer\n",
        "  from sumy.nlp.stemmers import Stemmer\n",
        "  from sumy.utils import get_stop_words\n",
        "\n",
        "\n",
        "  LANGUAGE = \"english\"\n",
        "  SENTENCES_COUNT = 10\n",
        "\n",
        "\n",
        "  parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
        "  # or for plain text files\n",
        "  # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
        "  stemmer = Stemmer(LANGUAGE)\n",
        "\n",
        "  summarizer = Summarizer(stemmer)\n",
        "  summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "  s=\"\"\n",
        "  for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
        "    s=s+str(sentence)\n",
        "  Luhn=(s)\n",
        "  return Luhn "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKXHLZcR9mfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Summary_generation(text,url):\n",
        "\n",
        "  #Generating Individual Summaries\n",
        "  Summary=[]\n",
        "  Summary.append(Kmeans_summary(text))\n",
        "  Summary.append(Textrank_Summary(text))\n",
        "  Summary.append(LSA_Summary(url))\n",
        "  Summary.append(Lexrank_Summary(url))\n",
        "  Summary.append(Reduction_Summary(url))\n",
        "  Summary.append(Luhn_summary(url))\n",
        "\n",
        "  Scores=[]\n",
        "  #Finding the Scores of The individual Summary\n",
        "  from rouge.rouge import rouge_n_sentence_level\n",
        "  target=Summary[1]\n",
        "  for i in range(0,len(Summary)):\n",
        "      recall, precision, rouge = rouge_n_sentence_level(Summary[i],target, 2)\n",
        "      Scores.append(rouge)\n",
        "  Key=sorted(range(len(Scores)), key=lambda i: Scores[i])[-3:]\n",
        "  \n",
        "  #Stacking the top three models\n",
        "  from gensim.summarization.summarizer import summarize\n",
        "  cols=['Kmeans F-measure', 'TextRank F-measure', 'LSA F-measure','Lex Rank F-measure', 'Reduction F-measure', 'Luhn F-measure']\n",
        "  s=\"\"\n",
        "  for i in range(0,len(Key)):\n",
        "    s=s+\"\"+Summary[Key[i]]\n",
        "  \n",
        "  #Final Summary\n",
        "  from gensim.summarization.summarizer import summarize\n",
        "  txt= '\\n'.join(s.split(\".\"))\n",
        "  txt=summarize(txt,word_count=40)\n",
        "  txt=txt.split(\"\\n\")\n",
        "  sentence=[]\n",
        "  for i in txt:\n",
        "    if i not in sentence:\n",
        "      sentence.append(i)\n",
        "    sent='.'.join(sentence)\n",
        "  return sent   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UYfTYMD9m1s",
        "colab_type": "code",
        "outputId": "03003fb0-7243-4ee2-8399-b014444cc3d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "Selected_Summary=[]\n",
        "for i in range(0,len(Selected_Text)):\n",
        "  Selected_Summary.append(Summary_generation(Selected_Text[i],Selected_link[i]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(112 unique tokens: ['john', 'backup', 'nw', 'recoveri', 'water']...) from 22 documents (total 314 corpus positions)\n",
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(239 unique tokens: ['new', 'aw', 'broad', 'dev', 'edg']...) from 44 documents (total 526 corpus positions)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(133 unique tokens: ['amazon', 'artifici', 'aw', 'base', 'big']...) from 28 documents (total 434 corpus positions)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(474 unique tokens: ['awesom', 'elasticsearch', 'fast', 'need', 'tell']...) from 270 documents (total 1393 corpus positions)\n",
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(200 unique tokens: ['cluster', 'elig', 'index', 'manag', 'master']...) from 71 documents (total 601 corpus positions)\n",
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(479 unique tokens: ['blog', 'gitlab', 'origin', 'post', 'publish']...) from 146 documents (total 1069 corpus positions)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(195 unique tokens: ['april', 'data', 'gitlab', 'manag', 'team']...) from 49 documents (total 493 corpus positions)\n",
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(602 unique tokens: ['jupyt', 'nbdev', 'notebook', 'us', 'chri']...) from 158 documents (total 1563 corpus positions)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(287 unique tokens: ['allow', 'call', 'complet', 'creat', 'document']...) from 42 documents (total 878 corpus positions)\n",
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(738 unique tokens: ['abstract', 'approach', 'field', 'limit', 'mandatori']...) from 110 documents (total 1278 corpus positions)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(424 unique tokens: ['answer', 'bryjngjolffson', 'coher', 'erik', 'expert']...) from 44 documents (total 958 corpus positions)\n",
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(621 unique tokens: ['artifici', 'chollet', 'françoi', 'gener', 'googl']...) from 154 documents (total 1476 corpus positions)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(307 unique tokens: ['abstract', 'achiev', 'arc', 'call', 'corpu']...) from 49 documents (total 795 corpus positions)\n",
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(632 unique tokens: ['car', 'curiou', 'cybertruck', 'design', 'histori']...) from 154 documents (total 1232 corpus positions)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
            "INFO:gensim.corpora.dictionary:built Dictionary(287 unique tokens: ['calcul', 'coordin', 'current', 'data', 'databa']...) from 49 documents (total 722 corpus positions)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykW_C1jkFO7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "NewsLetter = pd.DataFrame(\n",
        "    {'Title':Selected_title,\n",
        "      'Link':Selected_link,\n",
        "      'Summary':Selected_Summary,\n",
        "      'Date':Selected_date,\n",
        "      'Keywords': Selected_keywords,\n",
        "      'Source':  Selected_Source\n",
        "    })\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alWyt_FHddor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,len(NewsLetter)):\n",
        "  if NewsLetter.Summary[i]==\"\":\n",
        "    toi_article = Article(NewsLetter.Link[i], language=\"en\") # en for English \n",
        "    #To download the article \n",
        "    toi_article.download() \n",
        "    #To parse the article \n",
        "    toi_article.parse() \n",
        "    \n",
        "    #To perform natural language processing ie..nlp \n",
        "    toi_article.nlp()\n",
        "\n",
        "    NewsLetter.Summary[i]=toi_article.summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3aWb-FPd2oV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=NewsLetter.Summary[4].split(\"\\n\")\n",
        "del t[0]\n",
        "NewsLetter.Summary[4]=\".\".join(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJDYNx01b1YY",
        "colab_type": "code",
        "outputId": "1625e4f8-53b4-406b-a939-9121aa9a130a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "for i in range(0,len(NewsLetter.Summary)):\n",
        "  temp=NewsLetter.Summary[i].split(\"\\n\")\n",
        "  for j in range(0,len(temp)):\n",
        "    print()\n",
        "    if NewsLetter.Title[i]==temp[j]:\n",
        "      temp.remove(temp[j])\n",
        "      print(temp[j])\n",
        "  NewsLetter.Summary[i]=\".\".join(temp)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRTJcTnqZoLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "NewsLetter.to_csv('NewsLetter.csv')\n",
        "files.download('NewsLetter.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMmDKuts4Dyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import smtplib\n",
        "import email.message\n",
        "from jinja2 import Template\n",
        "server = smtplib.SMTP('smtp.gmail.com:587')\n",
        "\n",
        "email_content =\"\"\" <html>\n",
        " \n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
        "    \n",
        "   <title>Newsletter of Newsletters</title>\n",
        "   <style type=\"text/css\">\n",
        "    a {color: #d80a3e;}\n",
        "  body, #header h1, #header h2, p {margin: 0; padding: 0;}\n",
        "  #main {border: 1px solid #cfcece;}\n",
        "  img {display: block;}\n",
        "  #top-message p, #bottom p {color: #3f4042; font-size: 12px; font-family: Arial, Helvetica, sans-serif; }\n",
        "  #header h1 {color: #ffffff !important; font-family: \"Lucida Grande\", sans-serif; font-size: 24px; margin-bottom: 0!important; padding-bottom: 0; }\n",
        "  #header p {color: #ffffff !important; font-family: \"Lucida Grande\", \"Lucida Sans\", \"Lucida Sans Unicode\", sans-serif; font-size: 12px;  }\n",
        "  .news {padding-bottom: 2px;}\n",
        "  #content-4 h5 {\n",
        "      font-size: 18px; \n",
        "      color: #444444 !important; \n",
        "      font-family: Arial, Helvetica, sans-serif; \n",
        "      margin: 0 0 0.8em 0; \n",
        "      }\n",
        "  #content-4 h5 a {\n",
        "    height: 80px !important;\n",
        "    display: block;\n",
        "    overflow: hidden! important;\n",
        "    text-overflow: ellipsis! important;\n",
        "    }\n",
        "  #content-4 p { \n",
        "      font-size: 12px;\n",
        "      color: #444444!important;\n",
        "      font-family: \"Lucida Grande\",\"Lucida Sans\",\"Lucida Sans Unicode\",sans-serif;\n",
        "      line-height: 1.5;\n",
        "      overflow: hidden !important;\n",
        "      text-overflow: ellipsis !important;\n",
        "      max-height: 90px !important;\n",
        "      margin: 0 0 0.8em 0; \n",
        "      }\n",
        "   </style>\n",
        "</head>\n",
        " \n",
        "<body>\n",
        "\n",
        " \n",
        "<table id=\"main\" width=\"600\" align=\"center\" cellpadding=\"0\" cellspacing=\"15\" bgcolor=\"ffffff\">\n",
        "    <tr>\n",
        "      <td>\n",
        "        <table id=\"header\" cellpadding=\"10\" cellspacing=\"0\" align=\"center\" bgcolor=\"8fb3e9\">\n",
        "          <tr>\n",
        "            <td width=\"570\" align=\"center\"  bgcolor=\"#d80a3e\"><h1>Newsletter of Newsletters</h1></td>\n",
        "          </tr>\n",
        "          <tr>\n",
        "            <td width=\"570\" align=\"right\" bgcolor=\"#d80a3e\"></td>\n",
        "          </tr>\n",
        "        </table>\n",
        "      </td>\n",
        "    </tr>\n",
        " \n",
        "    <tr>\n",
        "      <td>\n",
        "        <table id=\"content-3\" cellpadding=\"0\" cellspacing=\"0\" align=\"center\">\n",
        "          <tr>\n",
        "            <td width=\"50%\" valign=\"top\" style=\"padding:5px 25px 5px 5px;\">\n",
        "              <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ0iv12vcOlIwmj9Irypthmhb6KH-FiNWadRDtwvn7rS4QF1fm2Gw&s\" width=\"250\" height=\"150\"  />\n",
        "            </td>\n",
        "            <td width=\"50%\" valign=\"top\" style=\"padding:5px;\">\n",
        "                <img src=\"https://www.elderresearch.com/hs-fs/hubfs/BLOG_Data%20Engineering%20with%20Discipline.jpg?width=850&name=BLOG_Data%20Engineering%20with%20Discipline.jpg\" width =\"250\" height=\"150\" />\n",
        "            </td>\n",
        "          </tr>\n",
        "        </table>\n",
        "      </td>\n",
        "    </tr>\n",
        "    \"\"\"\n",
        "\n",
        "body_start=\"\"\"<tr>\n",
        "      <td>\n",
        "        <table id=\"content-4\" cellpadding=\"0\" cellspacing=\"0\" align=\"center\">\n",
        "          <tr>\n",
        "            <td width=\"50%\" valign=\"top\" style=\"padding:5px;\">\"\"\"\n",
        "\n",
        "body_datascience=[]\n",
        "\n",
        "for i in range(0,len(NewsLetter)):\n",
        "  if NewsLetter[\"Source\"][i]=='Data Science':\n",
        "    t=Template(\"\"\"\n",
        "      <div class=\"news\">\n",
        "        <h5 class=\"newsLink\"><a href={{link}}>{{title}}</a></h5>\n",
        "        <p align=\"justify\">{{summary}}..</p>\n",
        "      </div>\n",
        "        \"\"\")\n",
        "    body_datascience.append(str(t.render(link=NewsLetter[\"Link\"][i],title=NewsLetter[\"Title\"][i],summary=NewsLetter[\"Summary\"][i],\n",
        "                                         source=NewsLetter[\"Source\"][i])))\n",
        "DS=\"\".join(body_datascience)\n",
        "\n",
        "body_seperater=\"\"\"</td>   \n",
        "            <td width=\"50%\" valign=\"top\" style=\"padding:5px;\">\"\"\"\n",
        "\n",
        "\n",
        "body_dataeng=[]\n",
        "for i in range(0,len(NewsLetter)):\n",
        "  if NewsLetter[\"Source\"][i] == 'Data Engineering':\n",
        "    t=Template(\"\"\"\n",
        "        <div class=\"news\">\n",
        "          <h5 class=\"newsLink\"><a href={{link}}>{{title}}</a></h5>\n",
        "          <p align=\"justify\">{{summary}}..</p>\n",
        "        </div>\n",
        "          \"\"\")\n",
        "    body_dataeng.append(str(t.render(link=NewsLetter[\"Link\"][i],title=NewsLetter[\"Title\"][i],summary=NewsLetter[\"Summary\"][i],source=NewsLetter[\"Source\"][i])))\n",
        "DE=\"\".join(body_dataeng)           \n",
        "\n",
        "body_end= \"\"\"</td>\n",
        "          </tr>\n",
        "        </table>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </table>\"\"\"\n",
        "  \n",
        "\n",
        "email_content=email_content+body_start+DS+body_seperater+DE+body_end+\"\"\"<table id=\"bottom\" cellpadding=\"20\" cellspacing=\"0\" width=\"600\" align=\"center\">\n",
        "    <tr>\n",
        "      <td align=\"center\">\n",
        "        <p>Stay ahead & Be relevant</p>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </table><!-- top message -->\n",
        "</td></tr></table><!-- wrapper -->\n",
        " \n",
        "</body>\n",
        "</html>\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9ufQX6SCMAt",
        "colab_type": "code",
        "outputId": "fbf68428-e492-4bc0-c6d7-d824242998d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "from smtplib import SMTP              # sending email\n",
        "from email.mime.text import MIMEText  # constructing messages\n",
        "\n",
        "from jinja2 import Environment        # Jinja2 templating\n",
        "\n",
        "msg = email.message.Message()\n",
        "msg['Subject'] = 'Newsletter of Newsletters'\n",
        " \n",
        "#email_content=\"hi\"\n",
        "msg['From'] = \"newsofnewsletter@gmail.com\"\n",
        "msg['To'] = \"newsofnewsletter@gmail.com\"\n",
        "password=\"Student#1899!\"\n",
        "msg.add_header('Content-Type', 'text/html')\n",
        "msg.set_payload((email_content).encode('ascii','ignore'))\n",
        " \n",
        "s = smtplib.SMTP('smtp.gmail.com: 587')\n",
        "s.starttls()\n",
        " \n",
        "# Login Credentials for sending the mail\n",
        "s.login(msg['From'], password)\n",
        " \n",
        "s.sendmail(msg['From'], msg[\"To\"], msg.as_string())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}